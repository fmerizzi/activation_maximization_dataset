{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Visualizing what convnets learn\n",
    "\n",
    "thanks to fchollet and the keras team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# The dimensions of our input image\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "# Our target layer: we will visualize the filters from this layer.\n",
    "# See `model.summary()` for list of layer names, if you want to change this.\n",
    "layer_name = \"block5_conv2\"\n",
    "filter_number = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Build a ResNet50V2 model loaded with pre-trained ImageNet weights\n",
    "model = keras.applications.VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Set up the gradient ascent process\n",
    "\n",
    "The \"loss\" we will maximize is simply the mean of the activation of a specific filter in\n",
    "our target layer. To avoid border effects, we exclude border pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss(input_image, filter_index):\n",
    "    activation = feature_extractor(input_image)\n",
    "    # We avoid border artifacts by only involving non-border pixels in the loss.\n",
    "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
    "    return tf.reduce_mean(filter_activation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Our gradient ascent function simply computes the gradients of the loss above\n",
    "with regard to the input image, and update the update image so as to move it\n",
    "towards a state that will activate the target filter more strongly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def gradient_ascent_step(img, filter_index, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img)\n",
    "        loss = compute_loss(img, filter_index)\n",
    "    # Compute gradients.\n",
    "    grads = tape.gradient(loss, img)\n",
    "    # Normalize gradients.\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    img += learning_rate * grads\n",
    "    return loss, img\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Set up the end-to-end filter visualization loop\n",
    "\n",
    "Our process is as follow:\n",
    "\n",
    "- Start from a random image that is close to \"all gray\" (i.e. visually netural)\n",
    "- Repeatedly apply the gradient ascent step function defined above\n",
    "- Convert the resulting input image back to a displayable form, by normalizing it,\n",
    "center-cropping it, and restricting it to the [0, 255] range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def initialize_image():\n",
    "    # We start from a gray image with some random noise\n",
    "    img = tf.random.uniform((1, img_width, img_height, 3))\n",
    "    # ResNet50V2 expects inputs in the range [-1, +1].\n",
    "    # Here we scale our random inputs to [-0.125, +0.125]\n",
    "    return (img - 0.5) * 0.25\n",
    "\n",
    "\n",
    "def visualize_filter(filter_index):\n",
    "    # We run gradient ascent for 20 steps\n",
    "    iterations = 30\n",
    "    learning_rate = 10.0\n",
    "    img = initialize_image()\n",
    "    for iteration in range(iterations):\n",
    "        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n",
    "\n",
    "    # Decode the resulting input image\n",
    "    img = deprocess_image(img[0].numpy())\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "def deprocess_image(img):\n",
    "    # Normalize array: center on 0., ensure variance is 0.15\n",
    "    img -= img.mean()\n",
    "    img /= img.std() + 1e-5\n",
    "    img *= 0.15\n",
    "\n",
    "    # Center crop\n",
    "    img = img[25:-25, 25:-25, :]\n",
    "\n",
    "    # Clip to [0, 1]\n",
    "    img += 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # Convert to RGB array\n",
    "    img *= 255\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's try it out with filter 0 in the target layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter 0\n",
      "filter 1\n",
      "filter 2\n",
      "filter 3\n",
      "filter 4\n",
      "filter 5\n",
      "filter 6\n",
      "filter 7\n",
      "filter 8\n",
      "filter 9\n",
      "filter 10\n",
      "filter 11\n",
      "filter 12\n",
      "filter 13\n",
      "filter 14\n",
      "filter 15\n",
      "filter 16\n",
      "filter 17\n",
      "filter 18\n",
      "filter 19\n",
      "filter 20\n",
      "filter 21\n",
      "filter 22\n",
      "filter 23\n",
      "filter 24\n",
      "filter 25\n",
      "filter 26\n",
      "filter 27\n",
      "filter 28\n",
      "filter 29\n",
      "filter 30\n",
      "filter 31\n",
      "filter 32\n",
      "filter 33\n",
      "filter 34\n",
      "filter 35\n",
      "filter 36\n",
      "filter 37\n",
      "filter 38\n",
      "filter 39\n",
      "filter 40\n",
      "filter 41\n",
      "filter 42\n",
      "filter 43\n",
      "filter 44\n",
      "filter 45\n",
      "filter 46\n",
      "filter 47\n",
      "filter 48\n",
      "filter 49\n",
      "filter 50\n",
      "filter 51\n",
      "filter 52\n",
      "filter 53\n",
      "filter 54\n",
      "filter 55\n",
      "filter 56\n",
      "filter 57\n",
      "filter 58\n",
      "filter 59\n",
      "filter 60\n",
      "filter 61\n",
      "filter 62\n",
      "filter 63\n",
      "filter 64\n",
      "filter 65\n",
      "filter 66\n",
      "filter 67\n",
      "filter 68\n",
      "filter 69\n",
      "filter 70\n",
      "filter 71\n",
      "filter 72\n",
      "filter 73\n",
      "filter 74\n",
      "filter 75\n",
      "filter 76\n",
      "filter 77\n",
      "filter 78\n",
      "filter 79\n",
      "filter 80\n",
      "filter 81\n",
      "filter 82\n",
      "filter 83\n",
      "filter 84\n",
      "filter 85\n",
      "filter 86\n",
      "filter 87\n",
      "filter 88\n",
      "filter 89\n",
      "filter 90\n",
      "filter 91\n",
      "filter 92\n",
      "filter 93\n",
      "filter 94\n",
      "filter 95\n",
      "filter 96\n",
      "filter 97\n",
      "filter 98\n",
      "filter 99\n",
      "filter 100\n",
      "filter 101\n",
      "filter 102\n",
      "filter 103\n",
      "filter 104\n",
      "filter 105\n",
      "filter 106\n",
      "filter 107\n",
      "filter 108\n",
      "filter 109\n",
      "filter 110\n",
      "filter 111\n",
      "filter 112\n",
      "filter 113\n",
      "filter 114\n",
      "filter 115\n",
      "filter 116\n",
      "filter 117\n",
      "filter 118\n",
      "filter 119\n",
      "filter 120\n",
      "filter 121\n",
      "filter 122\n",
      "filter 123\n",
      "filter 124\n",
      "filter 125\n",
      "filter 126\n",
      "filter 127\n",
      "filter 128\n",
      "filter 129\n",
      "filter 130\n",
      "filter 131\n",
      "filter 132\n",
      "filter 133\n",
      "filter 134\n",
      "filter 135\n",
      "filter 136\n",
      "filter 137\n",
      "filter 138\n",
      "filter 139\n",
      "filter 140\n",
      "filter 141\n",
      "filter 142\n",
      "filter 143\n",
      "filter 144\n",
      "filter 145\n",
      "filter 146\n",
      "filter 147\n",
      "filter 148\n",
      "filter 149\n",
      "filter 150\n",
      "filter 151\n",
      "filter 152\n",
      "filter 153\n",
      "filter 154\n",
      "filter 155\n",
      "filter 156\n",
      "filter 157\n",
      "filter 158\n",
      "filter 159\n",
      "filter 160\n",
      "filter 161\n",
      "filter 162\n",
      "filter 163\n",
      "filter 164\n",
      "filter 165\n",
      "filter 166\n",
      "filter 167\n",
      "filter 168\n",
      "filter 169\n",
      "filter 170\n",
      "filter 171\n",
      "filter 172\n",
      "filter 173\n",
      "filter 174\n",
      "filter 175\n",
      "filter 176\n",
      "filter 177\n",
      "filter 178\n",
      "filter 179\n"
     ]
    }
   ],
   "source": [
    "# Set up a model that returns the activation values for our target layer\n",
    "layer = model.get_layer(name=layer_name)\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=layer.output)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "for i in range(0,filter_number):\n",
    "    print(\"filter \" + str(i))\n",
    "    loss, img = visualize_filter(i)\n",
    "    path = \"./filterActivations/\"  + str(layer_name) + \"/filter-\" + str(i) + \".png\"\n",
    "    \n",
    "    keras.preprocessing.image.save_img(path, img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "This is what an input that maximizes the response of filter 0 in the target layer would\n",
    "look like:\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "visualizing_what_convnets_learn",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
